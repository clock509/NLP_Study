{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-4. 육아휴직 관련 법안에 대한 분석\n",
    " - import libraries\n",
    "  * nltk\n",
    "  * konlpy.corpus(kobill)\n",
    "  * konlpy.tag(Okt)\n",
    "  * matplotlib.pyplot\n",
    "  * matplotlib(font_manager, rc)\n",
    "  * platform\n",
    "  * wordcloud(WordCloud)\n",
    " - No base image file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from konlpy.corpus import kobill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1. 데이터 로딩\n",
    "files_ko = kobill.fileids()\n",
    "doc_ko = kobill.open('1809890.txt').read()  #Konlpy 내부문서 중 육아휴직 관련 텍스트.\n",
    "doc_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Okt 분석기로 명사 분석\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "tokens_ko = okt.nouns(doc_ko)\n",
    "\n",
    "tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3. 수집된 단어의 횟수와 고유한 횟수 확인\n",
    "\n",
    "ko = nltk.Text(tokens_ko, name = '대한민국 국회 의안 제1809890호')\n",
    "print(len(ko.tokens))  #수집된 단어의 횟수\n",
    "print(len(set(ko.tokens))) #고유한 횟수\n",
    "\n",
    "ko.vocab()     #각 단어의 빈도 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib로 한글 폰트 설정\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "\n",
    "path = \"C:/Windows/Fonts/malgun.ttf\"\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family = 'AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname = path).get_name()\n",
    "    rc('font', family = font_name)\n",
    "else:\n",
    "    print('Unknown system...')\n",
    "    \n",
    "#그래프가 외부창에서 그려짐.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#4. 그래프로 빈도 표현\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (12, 6))\n",
    "ko.plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. stopwords 지정.\n",
    "\n",
    "stopwords = ['.', ',', '(', ')', \"'\", '%', '-', 'X', ').', 'x', '의', '자', '에', '안', '번', '호',\n",
    "            '을', '이', '다', '만', '로', '가', '를', '생', '략', '및', '곧', '것임', '느', '취']\n",
    "ko = [each_word for each_word in ko if each_word not in stopwords] #ko에 있는 each_word 중 stopwords에 없는 것은 새 list인 ko에 넣어라.\n",
    "ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. stopwords가 반영된 plot 다시 그려보기.\n",
    "ko = nltk.Text(ko, name = '대한민국 국회 의안 제1809890호')\n",
    "print(ko)\n",
    "print(ko.vocab())\n",
    "plt.figure(figsize = (12, 6))\n",
    "ko.plot(50)    #출현 빈도 상위 50개 토큰\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. 특정 단어가 몇 번 언급되었는지 확인\n",
    "ko.count('지방공무원법')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. 원하는 단어의 문서 내 개략적인 위치 + 분량\n",
    "plt.figure(figsize = (12, 6))\n",
    "ko.dispersion_plot(['육아휴직', '초등학교', '공무원'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#9. 원하는 단어의 주변부 단어까지 같이 확인.\n",
    "print(ko.concordance('초등학교'))\n",
    "print()\n",
    "\n",
    "#연어(collocation) 파악 가능\n",
    "print(ko.collocations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. wordcloud todtjd\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "data = ko.vocab().most_common(100)   #최빈 100개 단어\n",
    "print(data)\n",
    "wordcloud = WordCloud(font_path = 'C:/Windows/Fonts/malgun.ttf',\n",
    "                     relative_scaling = 0.2,\n",
    "                     background_color = 'black',\n",
    "                     ).generate_from_frequencies(dict(data))\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-5. Naive Bayes Classifier의 이해(영문)\n",
    "- import libraries\n",
    "  * nltk.tokenize(word_tokenize)\n",
    "  * nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#심플한 분류기: Naive Bayes\n",
    "#지도학습의 한 종류.\n",
    "#두 사건을 서로 독립이라고 가정하고, 각각의 조건부확률을 활용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "#1. 연습용 데이터 4개 문장(문장, 긍/부정 label)\n",
    "train = [('i like you', 'pos'),\n",
    "         ('i hate you', 'neg'),\n",
    "         ('you like me', 'neg'),\n",
    "         ('i like her', 'pos')]  #i가 등장할 때 like는 긍정, like 자체만으로는 긍/부정 판단 불가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "all_words = set(word.lower() for sentence in train for word in word_tokenize(sentence[0]))\n",
    "all_words #train 문장에서 사용된 전체 단어를 찾는다.\n",
    "\n",
    "#all_words를 말뭉치라고 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in train]\n",
    "t  #말뭉치(all_words)를 기준으로 train 문장에 속한 단어인지 아닌지를 기록.\n",
    "   #True = 이 문장에 있는 단어, False = 이 문장에 없는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류기 실행\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features()\n",
    "# Train 문장에 붙은 긍/부정 태그를 이용하여 분류한 결과\n",
    "# hate라는 단어가 없을 때(=False일 때) 긍정일 비율이 1.7:1\n",
    "# like는 총 3번 사용되었고, 그 중 2개의 문장이 긍정의 의미였으므로 like가 있을 때(=True일 때) 긍정일 확률이 1.7:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 문장으로 나이브-베이즈 분류기 실행\n",
    "test_sentence = 'i like MeRui'\n",
    "test_sent_feature = {word.lower():\n",
    "                    (word in word_tokenize(test_sentence.lower())) for word in all_words}\n",
    "test_sent_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classify(test_sent_feature)    #테스트 문장 분류 결과: 긍정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-6. Naive Bayes Classifier의 이해(한글)\n",
    "- import libraries\n",
    "  * konlpy.tag(Okt)\n",
    "  * nltk.tokenize(word_tokenize)\n",
    "  * nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "pos_tagger = Okt()\n",
    "\n",
    "#영어와 달리, 한글은 형태소 분석을 반드시 해야 한다.\n",
    "#하지 않으면 어떻게 되는지 먼저 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train 문장 생성\n",
    "train = [('메리가 좋아', 'pos'),\n",
    "         ('고양이도 좋아', 'pos'),\n",
    "         ('난 수업이 지루해', 'neg'),\n",
    "         ('메리는 예쁜 고양이야', 'pos'),\n",
    "         ('난 마치고 메리랑 놀 거야', 'pos')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#말뭉치 만들기\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "all_words = set(word.lower() for sentence in train for word in word_tokenize(sentence[0]))\n",
    "all_words  #'고양이도', '고양이야'가 다른 단어로 잡힌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in train]\n",
    "t  #각 단어가 train문장에 속했는지 아닌지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#분류\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 문장\n",
    "test_sentence = '난 수업이 마치면 메리랑 놀 거야'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장만 보면 긍정적 결과가 나와야 할 것 같다.\n",
    "test_sent_feature = {word.lower(): word in word_tokenize(test_sentence.lower())\n",
    "                    for word in all_words}\n",
    "test_sent_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#그러나 실제로 적용하면, 'neg'를 출력한다.\n",
    "classifier.classify(test_sent_feature)\n",
    "\n",
    "#결국, 한글에서는 형태소 분석이 필요한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아래 함수처럼 tag를 붙여주는 것이 유리함.\n",
    "def tokenize(doc):\n",
    "    return ['/'.join(t) for t in pos_tagger.pos(doc, norm = True, stem = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위의 tokenize 함수를 사용해서 train 문장 분석.\n",
    "train_docs = [(tokenize(row[0]), row[1]) for row in train]\n",
    "train_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체 말뭉치 생성\n",
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#말뭉치에 있는 단어인지 아닌지를 구분하는 함수를 만들고 train 문장에 적용.\n",
    "#조사, 명사의 구분이 잘 되어있는 것이 특징.\n",
    "def term_exists(doc):\n",
    "    return {word: (word in set(doc)) for word in tokens}\n",
    "\n",
    "train_xy = [(term_exists(d), c) for d, c in train_docs]\n",
    "train_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#분류기 동작시키기!\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 문장으로 다시 테스트\n",
    "test_sentence = [(\"난 수업이 마치면 메리랑 놀 거야\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소분석 실시\n",
    "test_docs = pos_tagger.pos(test_sentence[0])\n",
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent_feature = {word: (word in tokens) for word in test_docs}\n",
    "test_sent_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 문장 분류 결과: pos\n",
    "classifier.classify(test_sent_feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
